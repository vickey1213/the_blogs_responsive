{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax tutorial\n",
    "\n",
    "This notebook is a terse tutorial walkthrough of the syntax of the `engine` language (which is based on the Handlebars templating language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextpy.ai import engine\n",
    "\n",
    "api_key = \"sk-api_key\"\n",
    "model_35_turbo = engine.llms.OpenAI(\"gpt-3.5-turbo\", chat_mode=True, api_key=api_key, caching=False)\n",
    "model_davinci = engine.llms.OpenAI(\"text-davinci-003\", chat_mode=False, api_key=api_key, caching=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic templating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = engine('''What is {{example}}?''')\n",
    "\n",
    "# this program has not been executed yet, so it still has the template placeholder in it\n",
    "program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we execute the program (by calling it) template placeholders are filled in\n",
    "# note that keyword arguments to the program become variables in the template namespace\n",
    "executed_program = program(example='truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the variables used by the program are returned as part of the executed program\n",
    "executed_program['example']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some variables we will use in the engine program\n",
    "people = ['John', 'Mary', 'Bob', 'Alice']\n",
    "ideas = [\n",
    "    {'name': 'truth', 'description': 'the state of being the case'},\n",
    "    {'name': 'love', 'description': 'a strong feeling of affection'}\n",
    "]\n",
    "\n",
    "# we can use the `each` block to iterate over a list\n",
    "program = engine('''List of people:\n",
    "{{#each people}}- {{this}}\n",
    "{{~! This is a comment. The ~ removes adjacent whitespace either before or after a tag, depending on where you place it}}\n",
    "{{/each~}}\n",
    "List of ideas:\n",
    "{{#each ideas}}{{this.name}}: {{this.description}}\n",
    "{{/each}}''')\n",
    "\n",
    "program(people=people, ideas=ideas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes (including engine programs inside other programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the program we will include\n",
    "program1 = engine('''List of people:\n",
    "{{#each people}}- {{this}}\n",
    "{{/each~}}''')\n",
    "\n",
    "# note that {{>prog_name}} is the same include syntax as in Handlebars\n",
    "program2 = engine('''{{>program1}}\n",
    "List of ideas:\n",
    "{{#each ideas}}{{this.name}}: {{this.description}}\n",
    "{{/each}}''')\n",
    "\n",
    "# we can pass program just like any other variable\n",
    "program2(program1=program1, people=people, ideas=ideas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text from an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the {{gen}} command to generate text from the language model\n",
    "# note that we used a ~ at the start of the command tag to remove the whitespace before it (just like in Handlebars)\n",
    "program = engine('''The best thing about the beach is {{~gen 'best' temperature=0.7 max_tokens=7}}''',llm = model_davinci)\n",
    "program()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flushing caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can flush a cache by calling the clear method\n",
    "# (this returns the number of items that were cleared)\n",
    "engine.llms.OpenAI.cache.clear()\n",
    "\n",
    "# you can also disable caching by passing caching=False to the LLM constructor\n",
    "# engine.llm = engine.llms.OpenAI(\"text-davinci-003\", caching=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting alternatives with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the {{#select}} command allows you to use the LLM to select from a set of options\n",
    "program = engine('''Is the following sentence offensive? Please answer with a single word, either \"Yes\", \"No\", or \"Maybe\".\n",
    "Sentence: {{example}}\n",
    "Answer:{{#select \"answer\" logprobs='logprobs'}} Yes{{or}} No{{or}} Maybe{{/select}}''',llm = model_davinci )\n",
    "executed_program = program(example='I hate tacos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the variables set by the program are returned as part of the executed program\n",
    "executed_program['logprobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_program['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the example above used a block version of the select command, but you can also\n",
    "# use a non-block version and just pass in a list of options\n",
    "options = [' Yes', ' No', ' Maybe']\n",
    "program = engine('''Is the following sentence offensive? Please answer with a single word, either \"Yes\", \"No\", or \"Maybe\".\n",
    "Sentence: {{example}}\n",
    "Answer:{{select \"answer\" options=options}}''',llm = model_davinci)\n",
    "executed_program = program(example='I hate tacos', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_program[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_program['response'], executed_program['answer']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is often useful to execute a part of the program, but then not include that part in later context\n",
    "# given to the language model. This can be done using the hidden=True argument. Several commands support\n",
    "# hidden=True, but here we use the {{#block}} command (which is just a generic block command that does\n",
    "# nothing other than what the arguments you pass to it do)\n",
    "program = engine('''{{#block hidden=True}}Generate a response to the following email:\n",
    "{{email}}.\n",
    "Response:{{gen \"response\"}}{{/block}}\n",
    "I will show you an email and a response, and you will tell me if it's offensive.\n",
    "Email: {{email}}.\n",
    "Response: {{response}}\n",
    "Is the response above offensive in any way? Please answer with a single word, either \"Yes\" or \"No\".\n",
    "Answer:{{#select \"answer\" logprobs='logprobs'}} Yes{{or}} No{{/select}}''',llm = model_davinci)\n",
    "\n",
    "executed_program = program(email='I hate tacos')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silent execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to run a program without displaying the output, you can use the silent=True argument\n",
    "executed_program = program(email='I hate tacos', silent=True)\n",
    "executed_program['answer']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating with `n>1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the {{gen}} command the n=number argument to generate multiple completions\n",
    "# only the first completion is used for future context, but the variable set\n",
    "# by the command is a list of all the completions, and you can interactively\n",
    "# click through each completion in the notebook visualization\n",
    "program = engine('''The best thing about the beach is{{gen 'best' n=3 temperature=0.7 max_tokens=7}}''',llm = model_davinci)\n",
    "executed_program = program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_program[\"best\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling custom user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the built in commands are functions from engine.library.* but you can also pass in your own functions\n",
    "def aggregate(best):\n",
    "    return '\\n'.join(['- ' + x for x in best])\n",
    "\n",
    "# note that we use hidden=True to prevent the {{gen}} command from being included in the output, and instead\n",
    "# just use the variable it sets as an input to the aggregate function\n",
    "program = engine('''The best thing about the beach is{{gen 'best' n=3 temperature=0.7 max_tokens=7 hidden=True}}\n",
    "{{aggregate best}}''',llm = model_davinci)\n",
    "executed_program = program(aggregate=aggregate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Await"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes you want to partially execute a program, the `await` command allows you to do this\n",
    "# it awaits a variable and then consumes that variables (so after the await command the variable)\n",
    "prompt = engine('''Generate a response to the following email:\n",
    "{{email}}.\n",
    "Response:{{gen \"response\"}}\n",
    "{{await 'instruction'}}\n",
    "{{gen 'updated_response'}}''', stream=True,llm = model_davinci )\n",
    "\n",
    "# note how the executed program is only partially executed, it stops at the await command\n",
    "# because the instruction variable is not yet set\n",
    "prompt = prompt(email='Hello there')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = prompt(instruction='Please translate the response above to Portuguese.',llm = model_davinci)\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = prompt(instruction='Please translate the response above to Chinese.')\n",
    "prompt2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use role based chat tags you need a chat model, here we use gpt-3.5-turbo but you can use 'gpt-4' as well\n",
    "engine.llm = engine.llms.OpenAI(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we enclose all of the text in one of the valid role tags for the model\n",
    "# `system`, `user`, and `assistant` are just shorthand for {{#role name=\"system\"}}...{{/role}}\n",
    "# the whitepace outside the role tags is ignored by gpt-4, the whitespace inside the role tags is not\n",
    "# so we use the ~ to remove the whitespace we don't want to give to the model (but want to keep in the code for clarity)\n",
    "program = engine('''\n",
    "{{#system~}}\n",
    "You are a helpful assistant.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "{{conversation_question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{! this is a comment. note that we don't have to use a stop=\"stop_string\" for the gen command below because engine infers the stop string from the role tag }}\n",
    "{{#assistant~}}\n",
    "{{gen 'response'}}\n",
    "{{~/assistant}}''',llm = model_35_turbo)\n",
    "\n",
    "executed_program = program(conversation_question='What is the meaning of life?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create and guide multi-turn conversations by using a series of role tags\n",
    "experts = engine('''\n",
    "{{#system~}}\n",
    "You are a helpful assistant.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "I want a response to the following question:\n",
    "{{query}}\n",
    "Who are 3 world-class experts (past or present) who would be great at answering this?\n",
    "Please don't answer the question or comment on it yet.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'experts' temperature=0 max_tokens=300}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\n",
    "In other words, their identity is not revealed, nor is the fact that there is a panel of experts answering the question.\n",
    "If the experts would disagree, just present their different positions as alternatives in the answer itself (e.g. 'some might argue... others might argue...').\n",
    "Please start your answer with ANSWER:\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0 max_tokens=500}}\n",
    "{{~/assistant}}''',llm = model_35_turbo)\n",
    "                   \n",
    "experts(query='What is the meaning of life?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want the model to have some inner dialog but then not include that dialog\n",
    "# in the context of later generations, you can use the {{#block}} command with hidden=True\n",
    "program = engine('''\n",
    "{{#system~}}\n",
    "You are a helpful assistant.\n",
    "{{~/system}}\n",
    "\n",
    "{{#block hidden=True}}\n",
    "{{#user~}}\n",
    "Please tell me a joke\n",
    "{{~/user}}\n",
    "\n",
    "{{! note that we don't have engine controls inside the assistant role because\n",
    "    the OpenAI API does not yet support that (Transformers chat models do) }}\n",
    "{{#assistant~}}\n",
    "{{gen 'joke'}}\n",
    "{{~/assistant}}\n",
    "{{~/block~}}\n",
    "\n",
    "{{#user~}}\n",
    "Is the following joke funny? Why or why not?\n",
    "{{joke}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'funny'}}\n",
    "{{~/assistant}}''',llm = model_35_turbo)\n",
    "program()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by putting an `await` inside a `geneach` loop you can create agents that consume some\n",
    "# varable, then do something and then wait for more content\n",
    "program = engine('''\n",
    "{{#system~}}\n",
    "You are a helpful assistant\n",
    "{{~/system}}\n",
    "\n",
    "{{~#geneach 'conversation' stop=False}}\n",
    "{{#user~}}\n",
    "{{set 'this.user_text' (await 'user_text')}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'this.ai_text' temperature=0 max_tokens=300}}\n",
    "{{~/assistant}}\n",
    "{{~/geneach}}''',llm = model_35_turbo)\n",
    "program = program(user_text ='hi there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we go through the loop we build up a conversation variable that contains the history of the conversation\n",
    "# note that the last entry in the conversation variable is empty because the `await` call happens before any\n",
    "# content is added to the `this` variable that represents the current item in the geneach loop\n",
    "program['conversation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we call the agent again and the loop continues, in this case building out a conversation\n",
    "program = program(user_text = 'What is the meaning of life?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program['conversation']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tools\n",
    "\n",
    "The example below uses a search engine (or a mock of one) to answer user questions. The whole system is defined in a single `engine` program, but you could also break it into multiple programs and `await` external calls if you don't want the engine program to control the whole process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_search(completion):\n",
    "    return '<search>' in completion\n",
    "\n",
    "def search(query):\n",
    "    # Fake search results\n",
    "    return [{'title': 'How do I cancel a Subscription? | Facebook Help Center',\n",
    "        'snippet': \"To stop a monthly Subscription to a creator: Go to the creator's Facebook Page using the latest version of the Facebook app for iOS, Android or from a computer. Select Go to Supporter Hub. Select . Select Manage Subscription to go to the iTunes or Google Play Store and cancel your subscription. Cancel your Subscription at least 24 hours before ...\"},\n",
    "        {'title': 'News | FACEBOOK Stock Price Today | Analyst Opinions - Insider',\n",
    "        'snippet': 'Stock | News | FACEBOOK Stock Price Today | Analyst Opinions | Markets Insider Markets Stocks Indices Commodities Cryptocurrencies Currencies ETFs News Facebook Inc (A) Cert Deposito Arg Repr...'},\n",
    "        {'title': 'Facebook Stock Price Today (NASDAQ: META) Quote, Market Cap, Chart ...',\n",
    "        'snippet': 'Facebook Stock Price Today (NASDAQ: META) Quote, Market Cap, Chart | WallStreetZen Meta Platforms Inc Stock Add to Watchlist Overview Forecast Earnings Dividend Ownership Statistics $197.81 +2.20 (+1.12%) Updated Mar 20, 2023 Meta Platforms shares are trading... find out Why META Price Moved with a free WallStreetZen account Why Price Moved'}]\n",
    "\n",
    "search_demo = engine('''Seach results:\n",
    "{{~#each results}}\n",
    "<result>\n",
    "{{this.title}}\n",
    "{{this.snippet}}\n",
    "</result>{{/each}}''')\n",
    "\n",
    "demo_results = [\n",
    "    {'title': 'OpenAI - Wikipedia', 'snippet': 'OpenAI systems run on the fifth most powerful supercomputer in the world. [5] [6] [7] The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Peter Thiel and others, [8] [1] [9] who collectively pledged US$ 1 billion. Musk resigned from the board in 2018 but remained a donor.'},\n",
    "    {'title': 'About - OpenAI', 'snippet': 'About OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity. Our vision for the future of AGI Our mission is to ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity. Read our plan for AGI'}, \n",
    "    {'title': 'Ilya Sutskever | Stanford HAI', 'snippet': '''Ilya Sutskever is Co-founder and Chief Scientist of OpenAI, which aims to build artificial general intelligence that benefits all of humanity. He leads research at OpenAI and is one of the architects behind the GPT models. Prior to OpenAI, Ilya was co-inventor of AlexNet and Sequence to Sequence Learning.'''}\n",
    "]\n",
    "\n",
    "s = search_demo(results=demo_results)\n",
    "\n",
    "practice_round = [\n",
    "    {'role': 'user', 'content' : 'Who are the founders of OpenAI?'},\n",
    "    {'role': 'assistant', 'content': '<search>Who are the founders of OpenAI</search>'},\n",
    "    {'role': 'user', 'content': str(search_demo(results=demo_results))},\n",
    "    {'role': 'assistant', 'content': 'The founders of OpenAI are Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Peter Thiel and others.'},\n",
    "]\n",
    "\n",
    "program = engine('''\n",
    "{{#system~}}\n",
    "You are a helpful assistant.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "From now on, whenever your response depends on any factual information, please search the web by using the function <search>query</search> before responding. I will then paste web results in, and you can respond.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Ok, I will do that. Let's do a practice round\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#each practice}}\n",
    "{{#if (== this.role \"user\")}}\n",
    "{{#user}}{{this.content}}{{/user}}\n",
    "{{else}}\n",
    "{{#assistant}}{{this.content}}{{/assistant}}\n",
    "{{/if}}\n",
    "{{/each}}\n",
    "\n",
    "{{#user~}}\n",
    "That was great, now let's do another one.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Sounds good\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "{{user_query}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen \"query\" stop=\"</search>\"}}{{#if (is_search query)}}</search>{{/if}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Search results: {{#each (search query)}}\n",
    "<result>\n",
    "{{this.title}}\n",
    "{{this.snippet}}\n",
    "</result>{{/each}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen \"answer\"}}\n",
    "{{~/assistant}}\n",
    "''',llm = model_35_turbo)\n",
    "\n",
    "query = \"What is Facebook's stock price right now?\"\n",
    "\n",
    "program = program(\n",
    "    user_query=query,\n",
    "    search=search,\n",
    "    is_search=is_search,\n",
    "    practice=practice_round\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 1px; opacity: 0.5; border: none; background: #cccccc;\">\n",
    "<div style=\"text-align: center; opacity: 0.5\">Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged!</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd8cabafb56980528edb83a46206c404687fdaed0bdad7c450ae020143ae38bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
